import pandas as pd

df = pd.read_csv('/content/wm013.csv', low_memory=False)

# Coerce columns to desired data type, e.g., float, with errors='coerce' to handle mixed types
columns_to_convert = [' WD_20_mean', ' WD_20_min', ' WD_20_max', ' WD_20_stdv']
df[columns_to_convert] = df[columns_to_convert].apply(pd.to_numeric, errors='coerce')
df
print(df.head())
print(df.info())
print(df.columns)

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import mean_squared_error
from datetime import datetime

# Ensure all visualizations are clear and readable
plt.rcParams['figure.figsize'] = (12, 6)
plt.rcParams['axes.grid'] = True
plt.rcParams.update({'font.size': 12})

import pandas as pd

# Remove any completely empty columns
df = df.dropna(axis=1, how='all')

# Check for missing values in the dataset
print(df.isnull().sum())

# Impute missing values using forward fill and backward fill
df.ffill(inplace=True)
df.bfill(inplace=True)

# Strip any leading or trailing whitespace from column names
df.columns = df.columns.str.strip()

# Ensure 'Tair_mean' column exists
if 'Tair_mean' not in df.columns:
    raise ValueError("The 'Tair_mean' column is missing from the dataset.")

# Ensure 'date_time' column exists
if 'date_time' not in df.columns:
    raise ValueError("The 'date_time' column is missing from the dataset.")

# Ensure date_time is in datetime format and handle any errors
df['date_time'] = pd.to_datetime(df['date_time'], errors='coerce')

# Drop rows with invalid date_time values
df.dropna(subset=['date_time'], inplace=True)

# Filter data for the year 2017
df_2017 = df[df['date_time'].dt.year == 2017]

# Check the filtered data
print(df_2017.info())

# Filter data from May 2017 to December 2017
df_2017 = df[(df['date_time'] >= '2017-05-01') & (df['date_time'] <= '2017-12-31')]

# Check the filtered data
print(df_2017.info())



import pandas as pd

df = pd.read_csv('/content/wm013.csv', low_memory=False)

# Coerce columns to desired data type, e.g., float, with errors='coerce' to handle mixed types
columns_to_convert = [' WD_20_mean', ' WD_20_min', ' WD_20_max', ' WD_20_stdv']
df[columns_to_convert] = df[columns_to_convert].apply(pd.to_numeric, errors='coerce')

             date_time  Station_ID   WS_62_mean   WS_62_min   WS_62_max  \
0  2017-07-01 00:10:00        WM13       4.1629      3.6380      4.5697   
1  2017-07-01 00:20:00        WM13       4.0387      3.6380      4.8803   
2  2017-07-01 00:30:00        WM13       3.7964      3.3274      4.2592   
3  2017-07-01 00:40:00        WM13       4.0927      3.6380      4.5697   
4  2017-07-01 00:50:00        WM13       4.1275      3.9486      4.5697   

    WS_62_stdv   WS_60_mean   WS_60_min   WS_60_max   WS_60_stdv  ...  \
0       0.1776       4.0932      3.6295      4.5606       0.1856  ...   
1       0.2149       3.9846      3.6295      4.5606       0.2080  ...   
2       0.1745       3.7462      3.3191      4.2503       0.1775  ...   
3       0.1627       4.0783      3.9399      4.2503       0.1546  ...   
4       0.1553       4.0969      3.9399      4.2503       0.1552  ...   

    Tgrad_stdv   Pbaro_mean   Pbaro_min   Pbaro_max   Pbaro_stdv   RH_mean  \
0        0.033      1009.28     1009.04     1009.52       0.0917      95.5   
1        0.039      1009.04     1009.04     1009.28       0.0720      95.6   
2        0.052      1009.04     1008.80     1009.28       0.0869      95.7   
3        0.057      1008.80     1008.80     1009.04       0.0850      96.3   
4        0.030      1008.80     1008.80     1009.04       0.0782      96.2   

    RH_min   RH_max   RH_stdv     
0     94.1     98.7     1.245     
1     94.3     98.8     1.136     
2     94.5     99.1     1.093     
3     94.8     99.4     1.191     
4     94.9     99.4     1.104     

[5 rows x 47 columns]
<class 'pandas.core.frame.DataFrame'>
RangeIndex: 105264 entries, 0 to 105263
Data columns (total 47 columns):
 #   Column       Non-Null Count   Dtype  
---  ------       --------------   -----  
 0   date_time    105264 non-null  object 
 1    Station_ID  105264 non-null  object 
 2    WS_62_mean  105264 non-null  float64
 3    WS_62_min   105264 non-null  float64
 4    WS_62_max   105264 non-null  float64
 5    WS_62_stdv  105264 non-null  float64
 6    WS_60_mean  105264 non-null  float64
 7    WS_60_min   105264 non-null  float64
 8    WS_60_max   105264 non-null  float64
 9    WS_60_stdv  105264 non-null  float64
 10   WS_40_mean  105264 non-null  float64
 11   WS_40_min   105264 non-null  float64
 12   WS_40_max   105264 non-null  float64
 13   WS_40_stdv  105264 non-null  float64
 14   WS_20_mean  105264 non-null  float64
 15   WS_20_min   105264 non-null  float64
 16   WS_20_max   105264 non-null  float64
 17   WS_20_stdv  105264 non-null  float64
 18   WS_10_mean  105264 non-null  float64
 19   WS_10_min   105264 non-null  float64
 20   WS_10_max   105264 non-null  float64
 21   WS_10_stdv  105264 non-null  float64
 22   WD_60_mean  105264 non-null  float64
 23   WD_60_min   105264 non-null  float64
 24   WD_60_max   105264 non-null  float64
 25   WD_60_stdv  105264 non-null  float64
 26   WD_20_mean  105259 non-null  float64
 27   WD_20_min   105259 non-null  float64
 28   WD_20_max   105259 non-null  float64
 29   WD_20_stdv  105259 non-null  float64
 30   Tair_mean   105264 non-null  float64
 31   Tair_min    105264 non-null  float64
 32   Tair_max    105264 non-null  float64
 33   Tair_stdv   105264 non-null  float64
 34   Tgrad_mean  105264 non-null  float64
 35   Tgrad_min   105264 non-null  float64
 36   Tgrad_max   105264 non-null  float64
 37   Tgrad_stdv  105264 non-null  float64
 38   Pbaro_mean  105264 non-null  float64
 39   Pbaro_min   105264 non-null  float64
 40   Pbaro_max   105264 non-null  float64
 41   Pbaro_stdv  105264 non-null  float64
 42   RH_mean     105264 non-null  float64
 43   RH_min      105264 non-null  float64
 44   RH_max      105264 non-null  float64
 45   RH_stdv     105264 non-null  float64
 46               105264 non-null  object 
dtypes: float64(44), object(3)
memory usage: 37.7+ MB
None

Index(['date_time', ' Station_ID', ' WS_62_mean', ' WS_62_min', ' WS_62_max',
       ' WS_62_stdv', ' WS_60_mean', ' WS_60_min', ' WS_60_max', ' WS_60_stdv',
       ' WS_40_mean', ' WS_40_min', ' WS_40_max', ' WS_40_stdv', ' WS_20_mean',
       ' WS_20_min', ' WS_20_max', ' WS_20_stdv', ' WS_10_mean', ' WS_10_min',
       ' WS_10_max', ' WS_10_stdv', ' WD_60_mean', ' WD_60_min', ' WD_60_max',
       ' WD_60_stdv', ' WD_20_mean', ' WD_20_min', ' WD_20_max', ' WD_20_stdv',
       ' Tair_mean', ' Tair_min', ' Tair_max', ' Tair_stdv', ' Tgrad_mean',
       ' Tgrad_min', ' Tgrad_max', ' Tgrad_stdv', ' Pbaro_mean', ' Pbaro_min',
       ' Pbaro_max', ' Pbaro_stdv', ' RH_mean', ' RH_min', ' RH_max',
       ' RH_stdv', ' '],
      dtype='object')

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import mean_squared_error
from datetime import datetime

# Ensure all visualizations are clear and readable
plt.rcParams['figure.figsize'] = (12, 6)
plt.rcParams['axes.grid'] = True
plt.rcParams.update({'font.size': 12})

date_time      0
 Station_ID    0
 WS_62_mean    0
 WS_62_min     0
 WS_62_max     0
 WS_62_stdv    0
 WS_60_mean    0
 WS_60_min     0
 WS_60_max     0
 WS_60_stdv    0
 WS_40_mean    0
 WS_40_min     0
 WS_40_max     0
 WS_40_stdv    0
 WS_20_mean    0
 WS_20_min     0
 WS_20_max     0
 WS_20_stdv    0
 WS_10_mean    0
 WS_10_min     0
 WS_10_max     0
 WS_10_stdv    0
 WD_60_mean    0
 WD_60_min     0
 WD_60_max     0
 WD_60_stdv    0
 WD_20_mean    0
 WD_20_min     0
 WD_20_max     0
 WD_20_stdv    0
 Tair_mean     0
 Tair_min      0
 Tair_max      0
 Tair_stdv     0
 Tgrad_mean    0
 Tgrad_min     0
 Tgrad_max     0
 Tgrad_stdv    0
 Pbaro_mean    0
 Pbaro_min     0
 Pbaro_max     0
 Pbaro_stdv    0
 RH_mean       0
 RH_min        0
 RH_max        0
 RH_stdv       0
               0
dtype: int64


<class 'pandas.core.frame.DataFrame'>
Index: 35137 entries, 0 to 105263
Data columns (total 47 columns):
 #   Column      Non-Null Count  Dtype         
---  ------      --------------  -----         
 0   date_time   35137 non-null  datetime64[ns]
 1   Station_ID  35137 non-null  object        
 2   WS_62_mean  35137 non-null  float64       
 3   WS_62_min   35137 non-null  float64       
 4   WS_62_max   35137 non-null  float64       
 5   WS_62_stdv  35137 non-null  float64       
 6   WS_60_mean  35137 non-null  float64       
 7   WS_60_min   35137 non-null  float64       
 8   WS_60_max   35137 non-null  float64       
 9   WS_60_stdv  35137 non-null  float64       
 10  WS_40_mean  35137 non-null  float64       
 11  WS_40_min   35137 non-null  float64       
 12  WS_40_max   35137 non-null  float64       
 13  WS_40_stdv  35137 non-null  float64       
 14  WS_20_mean  35137 non-null  float64       
 15  WS_20_min   35137 non-null  float64       
 16  WS_20_max   35137 non-null  float64       
 17  WS_20_stdv  35137 non-null  float64       
 18  WS_10_mean  35137 non-null  float64       
 19  WS_10_min   35137 non-null  float64       
 20  WS_10_max   35137 non-null  float64       
 21  WS_10_stdv  35137 non-null  float64       
 22  WD_60_mean  35137 non-null  float64       
 23  WD_60_min   35137 non-null  float64       
 24  WD_60_max   35137 non-null  float64       
 25  WD_60_stdv  35137 non-null  float64       
 26  WD_20_mean  35137 non-null  object        
 27  WD_20_min   35137 non-null  object        
 28  WD_20_max   35137 non-null  object        
 29  WD_20_stdv  35137 non-null  object        
 30  Tair_mean   35137 non-null  float64       
 31  Tair_min    35137 non-null  float64       
 32  Tair_max    35137 non-null  float64       
 33  Tair_stdv   35137 non-null  float64       
 34  Tgrad_mean  35137 non-null  float64       
 35  Tgrad_min   35137 non-null  float64       
 36  Tgrad_max   35137 non-null  float64       
 37  Tgrad_stdv  35137 non-null  float64       
 38  Pbaro_mean  35137 non-null  float64       
 39  Pbaro_min   35137 non-null  float64       
 40  Pbaro_max   35137 non-null  float64       
 41  Pbaro_stdv  35137 non-null  float64       
 42  RH_mean     35137 non-null  float64       
 43  RH_min      35137 non-null  float64       
 44  RH_max      35137 non-null  float64       
 45  RH_stdv     35137 non-null  float64       
 46              35137 non-null  object        
dtypes: datetime64[ns](1), float64(40), object(6)
memory usage: 12.9+ MB
None

#EDA for Tair_mean in 2017

# Plot histogram for the filtered period
plt.figure(figsize=(10, 6))
df_2017['Tair_mean'].hist(bins=30, color='skyblue')
plt.title('Histogram of Tair_mean (May 2017 - December 2017)')
plt.xlabel('Tair_mean')
plt.ylabel('Frequency')
plt.show()

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

# Load the dataset (assuming it's a CSV file)
try:
    # Replace with your dataset path
    df = pd.read_csv('/content/wm013.csv')
except FileNotFoundError:
    raise FileNotFoundError("The dataset file was not found. Please check the file path.")

# Handle empty columns: Drop columns that are completely empty
df = df.dropna(axis=1, how='all')

# Check and clean column names
df.columns = df.columns.str.strip()  # Remove any leading/trailing spaces
print("Columns in the dataset:")
print(df.columns)

# Check if 'date_time' column exists
if 'date_time' not in df.columns:
    raise KeyError("'date_time' column is missing from the dataset.")

# Convert 'date_time' to datetime format
df['date_time'] = pd.to_datetime(df['date_time'], errors='coerce')

# Ensure 'Tair_mean' column exists
if 'Tair_mean' not in df.columns:
    raise KeyError("'Tair_mean' column is missing from the dataset.")

# Filter data for the year 2017
df_2017 = df[df['date_time'].dt.year == 2017].copy()

# Filter for May 2017 only
df_2017_may = df_2017[(df_2017['date_time'].dt.month == 5) & (df_2017['date_time'].dt.year == 2017)]

# Filter for the first week of May 2017 (days 1 to 7)
df_2017_may_first_week = df_2017_may[df_2017_may['date_time'].dt.day <= 7]

# Check for missing values
missing_values = df_2017_may_first_week.isnull().sum()
print("Missing values per column:")
print(missing_values)

# Drop rows with missing 'Tair_mean' (if any)
df_2017_may_first_week = df_2017_may_first_week.dropna(subset=['Tair_mean'])

# Check if 'Tair_mean' column still exists after cleaning
print("Columns in the DataFrame after filtering:")
print(df_2017_may_first_week.columns)

if 'Tair_mean' not in df_2017_may_first_week.columns:
    raise KeyError("'Tair_mean' column is missing from the dataset after cleaning.")

# EDA: Plot 'Tair_mean' for the first week of May 2017
plt.figure(figsize=(12, 6))
sns.lineplot(data=df_2017_may_first_week, x='date_time', y='Tair_mean', marker='o')
plt.title('Tair_mean for the First Week of May 2017')
plt.xlabel('Date')
plt.ylabel('Tair_mean')
plt.grid(True)
plt.xticks(rotation=45)
plt.tight_layout()  # Ensure the layout fits into the figure area
plt.show()

# For further debugging: Print the first few rows of the DataFrame
print("Sample data from the DataFrame:")
print(df_2017_may_first_week.head())

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

# Load the dataset (assuming it's a CSV file)
try:
    # Replace with your dataset path
    df = pd.read_csv('/content/wm013.csv')
except FileNotFoundError:
    raise FileNotFoundError("The dataset file was not found. Please check the file path.")

# Handle empty columns: Drop columns that are completely empty
df = df.dropna(axis=1, how='all')

# Check and clean column names
df.columns = df.columns.str.strip()  # Remove any leading/trailing spaces
print("Columns in the dataset:")
print(df.columns)

# Check if 'date_time' column exists
if 'date_time' not in df.columns:
    raise KeyError("'date_time' column is missing from the dataset.")

# Convert 'date_time' to datetime format
df['date_time'] = pd.to_datetime(df['date_time'], errors='coerce')

# Ensure 'Tair_mean' column exists
if 'Tair_mean' not in df.columns:
    raise KeyError("'Tair_mean' column is missing from the dataset.")

# Filter data for the year 2017
df_2017 = df[df['date_time'].dt.year == 2017].copy()

# Filter for the period May 2017 to December 2017
df_2017 = df_2017[(df_2017['date_time'] >= '2017-05-01') & (df_2017['date_time'] <= '2017-12-31')]

# Keep rows for the first two weeks of each month
df_2017 = df_2017[df_2017['date_time'].dt.day <= 14]

# Check for missing values
missing_values = df_2017.isnull().sum()
print("Missing values per column:")
print(missing_values)

# Drop rows with missing 'Tair_mean' (if any)
df_2017 = df_2017.dropna(subset=['Tair_mean'])

# Check if 'Tair_mean' column still exists after cleaning
print("Columns in the DataFrame after filtering:")
print(df_2017.columns)

if 'Tair_mean' not in df_2017.columns:
    raise KeyError("'Tair_mean' column is missing from the dataset after cleaning.")

# EDA: Plot 'Tair_mean' over time
plt.figure(figsize=(12, 6))
sns.lineplot(data=df_2017, x='date_time', y='Tair_mean', marker='o')
plt.title('Tair_mean from May 2017 to December 2017 (First Two Weeks of Each Month)')
plt.xlabel('Date')
plt.ylabel('Tair_mean')
plt.grid(True)
plt.xticks(rotation=45)
plt.tight_layout()  # Ensure the layout fits into the figure area
plt.show()

# For further debugging: Print the first few rows of the DataFrame
print("Sample data from the DataFrame:")
print(df_2017.head())

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

# Load the dataset (assuming it's a CSV file)
try:
    # Replace with your dataset path
    df = pd.read_csv('/content/wm013.csv')
except FileNotFoundError:
    raise FileNotFoundError("The dataset file was not found. Please check the file path.")

# Handle empty columns: Drop columns that are completely empty
df = df.dropna(axis=1, how='all')

# Check and clean column names
df.columns = df.columns.str.strip()  # Remove any leading/trailing spaces
print("Columns in the dataset:")
print(df.columns)

# Check if 'date_time' column exists
if 'date_time' not in df.columns:
    raise KeyError("'date_time' column is missing from the dataset.")

# Convert 'date_time' to datetime format
df['date_time'] = pd.to_datetime(df['date_time'], errors='coerce')

# Ensure 'Tair_mean' column exists
if 'Tair_mean' not in df.columns:
    raise KeyError("'Tair_mean' column is missing from the dataset.")

# Filter data for the year 2017
df_2017 = df[df['date_time'].dt.year == 2017].copy()

# Filter for May 2017 only
df_2017_may = df_2017[(df_2017['date_time'].dt.month == 5) & (df_2017['date_time'].dt.year == 2017)]

# Check for missing values
missing_values = df_2017_may.isnull().sum()
print("Missing values per column:")
print(missing_values)

# Drop rows with missing 'Tair_mean' (if any)
df_2017_may = df_2017_may.dropna(subset=['Tair_mean'])

# Check if 'Tair_mean' column still exists after cleaning
print("Columns in the DataFrame after filtering:")
print(df_2017_may.columns)

if 'Tair_mean' not in df_2017_may.columns:
    raise KeyError("'Tair_mean' column is missing from the dataset after cleaning.")

# EDA: Plot 'Tair_mean' for May 2017
plt.figure(figsize=(12, 6))
sns.lineplot(data=df_2017_may, x='date_time', y='Tair_mean', marker='o')
plt.title('Tair_mean for May 2017')
plt.xlabel('Date')
plt.ylabel('Tair_mean')
plt.grid(True)
plt.xticks(rotation=45)
plt.tight_layout()  # Ensure the layout fits into the figure area
plt.show()

# For further debugging: Print the first few rows of the DataFrame
print("Sample data from the DataFrame:")
print(df_2017_may.head())

#The persistence model

import pandas as pd
import numpy as np
import plotly.graph_objects as go
from sklearn.metrics import mean_squared_error

# Assuming df_2017 is your DataFrame with 2017 data already filtered
df_2017_copy = df_2017.copy()

# Filter the DataFrame to keep only rows from May 2017
df_may_2017 = df_2017_copy[(df_2017_copy['date_time'] >= '2017-05-01') & (df_2017_copy['date_time'] < '2017-06-01')].copy()

# Add the persistence forecast column using .loc
df_may_2017.loc[:, 'Tair_mean_forecast'] = df_may_2017['Tair_mean'].shift(1)

# Drop NaNs introduced by shifting
df_may_2017 = df_may_2017.dropna(subset=['Tair_mean_forecast'])

# Calculate the Mean Squared Error for the persistence model
mse = mean_squared_error(df_may_2017['Tair_mean'], df_may_2017['Tair_mean_forecast'])
print(f"Mean Squared Error of the Persistence Model: {mse:.2f}")

# Create an interactive plot using Plotly
fig = go.Figure()

# Add the actual Tair_mean values to the plot
fig.add_trace(go.Scatter(x=df_may_2017['date_time'].values, y=df_may_2017['Tair_mean'],
                         mode='lines+markers', name='Actual', line=dict(color='blue')))

# Add the forecasted Tair_mean values to the plot
fig.add_trace(go.Scatter(x=df_may_2017['date_time'].values, y=df_may_2017['Tair_mean_forecast'],
                         mode='lines+markers', name='Forecast', line=dict(color='orange', dash='dash')))

# Update layout for better visualization
fig.update_layout(title='Persistence Model: Actual vs Forecast (May 2017)',
                  xaxis_title='Date',
                  yaxis_title='Tair_mean',
                  legend_title='Legend',
                  hovermode='x unified')

# Show the interactive plot
fig.show()

import pandas as pd
import numpy as np
import plotly.graph_objects as go
from plotly.subplots import make_subplots
from sklearn.metrics import mean_squared_error

# Assuming df_2017_copy is already defined and contains the necessary columns
# Add the persistence forecast column (make sure it exists)
if 'Tair_mean_forecast' not in df_2017_copy.columns:
    df_2017_copy['Tair_mean_forecast'] = df_2017_copy['Tair_mean'].shift(1)

# Drop NaNs introduced by shifting (optional based on your requirements)
df_2017_copy = df_2017_copy.dropna(subset=['Tair_mean_forecast'])

# Filter the dataset to include only rows from the first week of May 2017
df_may_first_week = df_2017_copy[(df_2017_copy['date_time'] >= '2017-05-01') & (df_2017_copy['date_time'] < '2017-05-08')]

# Check if the filtered DataFrame contains the necessary columns
if 'Tair_mean_forecast' not in df_may_first_week.columns:
    raise KeyError("'Tair_mean_forecast' column is missing in the filtered DataFrame.")

# Convert 'date_time' to numpy array to handle future deprecation issues
date_time_values = df_may_first_week['date_time'].to_numpy()

# Create subplots with two rows and one column
fig = make_subplots(rows=2, cols=1, shared_xaxes=True, vertical_spacing=0.1,
                    subplot_titles=('Actual Tair_mean', 'Forecasted Tair_mean'))

# Add actual values to the first subplot
fig.add_trace(go.Scatter(x=date_time_values, y=df_may_first_week['Tair_mean'],
                         mode='lines+markers', name='Actual', line=dict(color='blue')),
              row=1, col=1)

# Add forecasted values to the second subplot
fig.add_trace(go.Scatter(x=date_time_values, y=df_may_first_week['Tair_mean_forecast'],
                         mode='lines+markers', name='Forecast', line=dict(dash='dash', color='orange'),
                         marker=dict(symbol='cross', color='red')),
              row=2, col=1)

# Update layout for better readability
fig.update_layout(title_text='Persistence Model: Actual vs Forecast (First Week of May 2017)',
                  xaxis_title='Date', yaxis_title='Temperature',
                  xaxis2_title='Date', yaxis2_title='Temperature',
                  showlegend=True)

# Show the interactive plot
fig.show()

#MODEL ACCURACY

import pandas as pd
import numpy as np
from sklearn.metrics import mean_squared_error, mean_absolute_error

# Assuming df_2017 is already defined and filtered for 2017

# Create a copy of the DataFrame to avoid modifying the original DataFrame
df_2017_copy = df_2017.copy()

# Add the persistence forecast column
df_2017_copy['Tair_mean_forecast'] = df_2017_copy['Tair_mean'].shift(1)

# Drop NaNs introduced by shifting
df_2017_copy = df_2017_copy.dropna(subset=['Tair_mean_forecast'])

# Calculate the Mean Absolute Error (MAE) for the persistence model
mae = mean_absolute_error(df_2017_copy['Tair_mean'], df_2017_copy['Tair_mean_forecast'])

# Calculate the Mean Squared Error (MSE) for the persistence model
mse = mean_squared_error(df_2017_copy['Tair_mean'], df_2017_copy['Tair_mean_forecast'])

# Calculate the Root Mean Squared Error (RMSE)
rmse = np.sqrt(mse)

# Print the accuracy metrics
print(f"Mean Absolute Error (MAE): {mae:.2f}")
print(f"Mean Squared Error (MSE): {mse:.2f}")
print(f"Root Mean Squared Error (RMSE): {rmse:.2f}")

# Create a copy of the DataFrame to avoid modifying the original DataFrame
df_2017_copy = df_2017.copy()

# Apply rolling mean to smooth the data (7-day window)
df_2017_copy.loc[:, 'Tair_mean_smooth'] = df_2017_copy['Tair_mean'].rolling(window=7).mean()

# Shift the smoothed data for the persistence model
df_2017_copy.loc[:, 'Tair_mean_forecast_smooth'] = df_2017_copy['Tair_mean_smooth'].shift(1)

# Drop NaNs introduced by smoothing
df_2017_copy = df_2017_copy.dropna(subset=['Tair_mean_forecast_smooth'])

# Removing outliers (values more than 3 standard deviations from the mean)
df_2017 = df_2017[np.abs(df_2017['Tair_mean'] - df_2017['Tair_mean'].mean()) <= (3 * df_2017['Tair_mean'].std())]

# Print the columns of the DataFrame to verify their names
print(df_may_first_week.columns)

import pandas as pd
import numpy as np
import plotly.graph_objects as go
from plotly.subplots import make_subplots

# Assuming df_may_first_week is already defined and filtered for May 2017

# Create a smoothed forecast column with placeholder data
df_may_first_week.loc[:, 'Tair_mean_forecast_smooth'] = df_may_first_week['Tair_mean'].rolling(window=3).mean()

# Convert 'date_time' to numpy array to handle future deprecation issues
date_time_values = df_may_first_week['date_time'].to_numpy()

# Create a subplot with 2 rows and 1 column
fig = make_subplots(rows=2, cols=1, shared_xaxes=True, vertical_spacing=0.1,
                    subplot_titles=('Actual Tair_mean (First Week of May 2017)',
                                    'Smoothed Forecast Tair_mean (First Week of May 2017)'))

# Add actual Tair_mean to the first subplot
fig.add_trace(go.Scatter(x=date_time_values,
                         y=df_may_first_week['Tair_mean'],
                         mode='lines+markers',
                         name='Actual',
                         line=dict(color='blue')),
              row=1, col=1)

# Add smoothed forecast Tair_mean to the second subplot
fig.add_trace(go.Scatter(x=date_time_values,
                         y=df_may_first_week['Tair_mean_forecast_smooth'],
                         mode='lines+markers',
                         name='Smoothed Forecast',
                         line=dict(color='orange', dash='dash')),
              row=2, col=1)

# Update layout
fig.update_layout(height=800, width=1000,
                  title_text='Actual vs Smoothed Forecast (First Week of May 2017)',
                  xaxis=dict(title='Date'),
                  yaxis=dict(title='Tair_mean'),
                  legend_title_text='Legend')

# Show the interactive plot
fig.show()

from sklearn.metrics import r2_score

# Ensure the DataFrame used here has the necessary forecast columns
# Using df_2017_copy where columns are created
# Make sure df_2017_copy contains 'Tair_mean_forecast' and 'Tair_mean_forecast_smooth'

# Calculate R-squared for the original and improved models
r2_original = r2_score(df_2017_copy['Tair_mean'], df_2017_copy['Tair_mean_forecast'])
r2_improved = r2_score(df_2017_copy['Tair_mean'], df_2017_copy['Tair_mean_forecast_smooth'])

# Print the R-squared values
print(f"R-squared (Original Model): {r2_original:.2f}")
print(f"R-squared (Improved Model): {r2_improved:.2f}")

Convert to Classification Problem

# Define bins for classification
bins = [-np.inf, 0, 10, 20, 30, np.inf]
labels = ['Very Low', 'Low', 'Moderate', 'High', 'Very High']

# Use df_2017_copy if 'Tair_mean_forecast_smooth' is created there
df_2017_copy['Tair_mean_bin'] = pd.cut(df_2017_copy['Tair_mean'], bins=bins, labels=labels)
df_2017_copy['Tair_mean_forecast_bin'] = pd.cut(df_2017_copy['Tair_mean_forecast_smooth'], bins=bins, labels=labels)

# Drop NaNs introduced by binning
df_2017_copy.dropna(subset=['Tair_mean_bin', 'Tair_mean_forecast_bin'], inplace=True)

# If you need to use df_2017 for other purposes, ensure columns are properly included there
# df_2017 = df_2017_copy.copy()  # Optional: Update df_2017 if needed

#Calculate Classification Metrics

from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score

# Ensure you're using the DataFrame where binning columns are available
# Use df_2017_copy if columns were created there
if 'Tair_mean_forecast_bin' in df_2017_copy.columns and 'Tair_mean_bin' in df_2017_copy.columns:
    # Calculate precision, recall, and F1-score
    precision = precision_score(df_2017_copy['Tair_mean_bin'], df_2017_copy['Tair_mean_forecast_bin'], average='weighted')
    recall = recall_score(df_2017_copy['Tair_mean_bin'], df_2017_copy['Tair_mean_forecast_bin'], average='weighted')
    f1 = f1_score(df_2017_copy['Tair_mean_bin'], df_2017_copy['Tair_mean_forecast_bin'], average='weighted')
    accuracy = accuracy_score(df_2017_copy['Tair_mean_bin'], df_2017_copy['Tair_mean_forecast_bin'])

    # Print the classification metrics
    print(f"Precision: {precision:.2f}")
    print(f"Recall: {recall:.2f}")
    print(f"F1-Score: {f1:.2f}")
    print(f"Accuracy: {accuracy:.2f}")
else:
    print("Required columns are missing from the DataFrame.")


